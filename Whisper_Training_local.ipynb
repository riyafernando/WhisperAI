{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riyafernando/WhisperAI/blob/main/Whisper_Training_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d406ef56",
      "metadata": {
        "id": "d406ef56",
        "outputId": "e9ff1b33-13ed-4139-934c-b01f42109909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
            "Your token has been saved to /Users/riyafernando/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5674ef92",
      "metadata": {
        "id": "5674ef92",
        "outputId": "a91e3527-bdcb-48f0-9689-33e53b9dcf35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset common_voice_11_0 (/Users/riyafernando/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/hi/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n",
            "Found cached dataset common_voice_11_0 (/Users/riyafernando/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/hi/11.0.0/2c65b95d99ca879b1b1074ea197b65e0497848fd697fdb0582e0f6b75b6f4da0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 6\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
            "        num_rows: 6\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", use_auth_token=True)\n",
        "common_voice[\"train\"] = common_voice[\"train\"].select([0, 10, 20, 30, 40, 50])\n",
        "common_voice[\"test\"] = common_voice[\"test\"].select([0, 10, 20, 30, 40, 50])\n",
        "print(common_voice)\n",
        "\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18caf0c",
      "metadata": {
        "id": "f18caf0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9815d5d7",
      "metadata": {
        "id": "9815d5d7"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")\n",
        "\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Hindi\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5892211",
      "metadata": {
        "id": "b5892211",
        "outputId": "e9e76a7e-0e3f-450d-a5f3-c089a611e1b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riyafernando/miniconda3/lib/python3.10/site-packages/datasets/features/audio.py:313: UserWarning: \n",
            "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
            "\n",
            "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
            "\n",
            "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
            "\n",
            "\tpip install \"torchaudio<0.12\"`.\n",
            "\n",
            "Otherwise 'mp3' files will be decoded with `librosa`.\n",
            "  warnings.warn(\n",
            "/Users/riyafernando/miniconda3/lib/python3.10/site-packages/datasets/features/audio.py:334: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
            "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
          ]
        }
      ],
      "source": [
        "#dont run\n",
        "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
        "labels = tokenizer(input_str).input_ids\n",
        "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
        "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input:                 {input_str}\")\n",
        "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
        "print(f\"Decoded w/out special: {decoded_str}\")\n",
        "print(f\"Are equal:             {input_str == decoded_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788a479f",
      "metadata": {
        "scrolled": true,
        "id": "788a479f",
        "outputId": "bf5dcbf0-ef13-445b-adec-e1caa3245c26"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/riyafernando/miniconda3/lib/python3.10/site-packages/datasets/features/audio.py:313: UserWarning: \n",
            "To support 'mp3' decoding with `torchaudio>=0.12.0`, please install `ffmpeg4` system package. On Google Colab you can run:\n",
            "\n",
            "\t!add-apt-repository -y ppa:jonathonf/ffmpeg-4 && apt update && apt install -y ffmpeg\n",
            "\n",
            "and restart your runtime. Alternatively, you can downgrade `torchaudio`:\n",
            "\n",
            "\tpip install \"torchaudio<0.12\"`.\n",
            "\n",
            "Otherwise 'mp3' files will be decoded with `librosa`.\n",
            "  warnings.warn(\n",
            "/Users/riyafernando/miniconda3/lib/python3.10/site-packages/datasets/features/audio.py:334: UserWarning: Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\n",
            "  warnings.warn(\"Decoding mp3 with `librosa` instead of `torchaudio`, decoding might be slow.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': '/Users/riyafernando/.cache/huggingface/datasets/downloads/extracted/63f3f46acdfdc6d76acfd73cc151c1a944510ab31e939af540da2d0bbc0a5300/common_voice_hi_26008353.mp3', 'array': array([], dtype=float32), 'sampling_rate': 48000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Hindi\", task=\"transcribe\")\n",
        "\n",
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdeb6d8",
      "metadata": {
        "id": "9bdeb6d8",
        "outputId": "6237dc42-d14c-4f23-c359-9d6e60e70be3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'audio': {'path': '/Users/riyafernando/.cache/huggingface/datasets/downloads/extracted/63f3f46acdfdc6d76acfd73cc151c1a944510ab31e939af540da2d0bbc0a5300/common_voice_hi_26008353.mp3', 'array': array([], dtype=float32), 'sampling_rate': 16000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bee226d",
      "metadata": {
        "id": "9bee226d"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77313abb",
      "metadata": {
        "id": "77313abb"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(remove_columns = common_voice.column_names[\"train\"])\n",
        "common_voice = common_voice.map(prepare_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55750a1a",
      "metadata": {
        "scrolled": true,
        "id": "55750a1a",
        "outputId": "8f130295-4b7a-400f-8c2e-8c3bc3f38a72",
        "colab": {
          "referenced_widgets": [
            "328862a7bcd742a5b4e5c09ec30ed8d7"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "328862a7bcd742a5b4e5c09ec30ed8d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#dont run\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dad1a4d",
      "metadata": {
        "id": "0dad1a4d"
      },
      "outputs": [],
      "source": [
        "# prepped for training\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df232819",
      "metadata": {
        "id": "df232819"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f9ddf6",
      "metadata": {
        "id": "c4f9ddf6",
        "outputId": "b9ebbb79-1bbe-495f-ba08-594e152a6e38"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'evaluate'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#EVALUTION Metrics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      4\u001b[0m metric \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(pred):\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
          ]
        }
      ],
      "source": [
        "#EVALUTION Metrics\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc4349ef",
      "metadata": {
        "id": "cc4349ef"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff8a191",
      "metadata": {
        "id": "fff8a191"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2, #changed from 8 to 2\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649455d0",
      "metadata": {
        "id": "649455d0"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1113e585",
      "metadata": {
        "id": "1113e585"
      },
      "outputs": [],
      "source": [
        "# Start Training\n",
        "trainer.train()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}